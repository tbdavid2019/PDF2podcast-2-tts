import io
import os
from pathlib import Path
from tempfile import NamedTemporaryFile
import time
import gradio as gr
from openai import OpenAI
from pydub import AudioSegment
from dotenv import load_dotenv

# åŠ è¼‰ç’°å¢ƒè®Šé‡
load_dotenv()

# ç²å– OpenAI API Key (å¦‚æœåœ¨ç’°å¢ƒè®Šé‡ä¸­è¨­ç½®äº†)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")

# æ¨™æº–éŸ³é »æ¨¡å‹å’Œè²éŸ³é¸é …
STANDARD_AUDIO_MODELS = [
    "gpt-4o-mini-tts",
    "gpt-4o-audio-preview",
    "tts-1",
    "tts-1-hd",
]
STANDARD_VOICES = [
    "alloy",
    "echo",
    "fable",
    "onyx",
    "nova",
    "shimmer",
    "coral",
    "sage",
]

# å„ªåŒ–è…³æœ¬è™•ç† - åˆä¸¦ç›¸åŒèªªè©±è€…é€£çºŒæ–‡æœ¬
def optimize_script(script):
    print("ğŸ”„ é–‹å§‹å„ªåŒ–è…³æœ¬è™•ç†...")
    lines = [line.strip() for line in script.splitlines() if line.strip()]
    optimized = []
    current_speaker = None
    current_text = ""
    
    for line in lines:
        if line.lower().startswith("speaker-1:"):
            speaker = "speaker-1"
            text = line.split(":", 1)[1].strip()
        elif line.lower().startswith("speaker-2:"):
            speaker = "speaker-2"
            text = line.split(":", 1)[1].strip()
        else:
            speaker = "speaker-1"  # é»˜èªä½¿ç”¨èªªè©±è€…1
            text = line
        
        # å¦‚æœèªªè©±è€…è®Šäº†ï¼Œä¿å­˜ä¹‹å‰çš„æ–‡æœ¬ä¸¦é–‹å§‹æ–°çš„
        if speaker != current_speaker and current_text:
            optimized.append((current_speaker, current_text))
            current_text = text
            current_speaker = speaker
        else:
            # ç›¸åŒèªªè©±è€…ï¼Œåˆä¸¦æ–‡æœ¬ï¼ˆåŠ ç©ºæ ¼ï¼‰
            if current_text:
                current_text += " " + text
            else:
                current_text = text
                current_speaker = speaker
                
    # æ·»åŠ æœ€å¾Œä¸€å€‹èªªè©±è€…çš„æ–‡æœ¬
    if current_text:
        optimized.append((current_speaker, current_text))
        
    print(f"âœ… è…³æœ¬å„ªåŒ–å®Œæˆï¼Œå…± {len(optimized)} æ®µå°è©±")
    return optimized

def get_mp3(text: str, voice: str, audio_model: str, audio_api_key: str, instructions: str = None) -> bytes:
    """ä½¿ç”¨ OpenAI TTS API ç”ŸæˆéŸ³é »"""
    print(f"ğŸ¤ é–‹å§‹ç”ŸæˆéŸ³é »: é•·åº¦ {len(text)} å­—ç¬¦, è²éŸ³: {voice}, æ¨¡å‹: {audio_model}")
    
    # æª¢æŸ¥æ–‡æœ¬é•·åº¦ï¼ŒOpenAI TTS API æœ‰ 4096 å€‹æ¨™è¨˜çš„é™åˆ¶
    # å¤§ç´„ 1000 å€‹æ¼¢å­—ç´„ç­‰æ–¼ 2000-3000 å€‹æ¨™è¨˜ï¼Œç‚ºå®‰å…¨èµ·è¦‹ï¼Œæˆ‘å€‘å°‡é™åˆ¶è¨­ç‚º 1000 å€‹å­—ç¬¦
    MAX_TEXT_LENGTH = 1000
    
    client = OpenAI(api_key=audio_api_key)
    
    # å¦‚æœæ–‡æœ¬é•·åº¦è¶…éé™åˆ¶ï¼Œåˆ†å‰²æ–‡æœ¬
    if len(text) > MAX_TEXT_LENGTH:
        print(f"ğŸ“ æ–‡æœ¬éé•· ({len(text)} å­—ç¬¦)ï¼Œåˆ†å‰²æˆå¤šå€‹å€å¡Š")
        # å°‡æ–‡æœ¬åˆ†å‰²æˆæ›´å°çš„å¡Š
        text_chunks = []
        for i in range(0, len(text), MAX_TEXT_LENGTH):
            text_chunks.append(text[i:i + MAX_TEXT_LENGTH])
        
        print(f"ğŸ“¦ å…±åˆ†å‰²æˆ {len(text_chunks)} å€‹å€å¡Š")
        
        # ç‚ºæ¯å€‹å¡Šç”ŸæˆéŸ³é »ä¸¦åˆä¸¦
        combined_audio = b""
        for i, chunk in enumerate(text_chunks, 1):
            print(f"ğŸ”„ è™•ç†å€å¡Š {i}/{len(text_chunks)}: {len(chunk)} å­—ç¬¦")
            try:
                # æ§‹å»º API åƒæ•¸
                api_params = {
                    "model": audio_model,
                    "voice": voice,
                    "input": chunk,
                }
                if instructions:
                    api_params["instructions"] = instructions
                    print(f"ğŸ’¬ ä½¿ç”¨èªæ°£æŒ‡ç¤º: {instructions}")
                
                print(f"ğŸ“¡ èª¿ç”¨ OpenAI TTS API...")
                with client.audio.speech.with_streaming_response.create(**api_params) as response:
                    with io.BytesIO() as file:
                        for audio_chunk in response.iter_bytes():
                            file.write(audio_chunk)
                        chunk_audio = file.getvalue()
                        combined_audio += chunk_audio
                        print(f"âœ… å€å¡Š {i} ç”Ÿæˆå®Œæˆ: {len(chunk_audio)} bytes")
            except Exception as e:
                print(f"âŒ å€å¡Š {i} ç”Ÿæˆå¤±æ•—: {e}")
                raise
        
        print(f"ğŸµ æ‰€æœ‰å€å¡Šåˆä¸¦å®Œæˆï¼Œç¸½å¤§å°: {len(combined_audio)} bytes")
        return combined_audio
    else:
        # åŸå§‹é‚è¼¯ï¼Œè™•ç†çŸ­æ–‡æœ¬
        try:
            # æ§‹å»º API åƒæ•¸
            api_params = {
                "model": audio_model,
                "voice": voice,
                "input": text,
            }
            if instructions:
                api_params["instructions"] = instructions
                print(f"ğŸ’¬ ä½¿ç”¨èªæ°£æŒ‡ç¤º: {instructions}")
            
            print(f"ğŸ“¡ èª¿ç”¨ OpenAI TTS API...")
            with client.audio.speech.with_streaming_response.create(**api_params) as response:
                with io.BytesIO() as file:
                    for audio_chunk in response.iter_bytes():
                        file.write(audio_chunk)
                    audio_data = file.getvalue()
                    print(f"âœ… éŸ³é »ç”Ÿæˆå®Œæˆ: {len(audio_data)} bytes")
                    return audio_data
        except Exception as e:
            print(f"âŒ éŸ³é »ç”Ÿæˆå¤±æ•—: {e}")
            raise

def generate_audio_from_script(
    script: str,
    audio_api_key: str,
    audio_model: str = "gpt-4o-mini-tts",
    speaker1_voice: str = "onyx",
    speaker2_voice: str = "nova",
    volume_boost: float = 0,
    speaker1_instructions: str = "ä¿æŒæ´»æ½‘æ„‰å¿«çš„èªæ°£",
    speaker2_instructions: str = "ä¿æŒæ´»æ½‘æ„‰å¿«çš„èªæ°£",
) -> tuple[bytes, str]:
    """å¾è…³æœ¬ç”ŸæˆéŸ³é »ï¼Œæ”¯æŒå…©å€‹èªªè©±è€…ï¼Œä¸¦å„ªåŒ– API èª¿ç”¨"""
    print("ğŸ¬ é–‹å§‹å¾è…³æœ¬ç”ŸæˆéŸ³é »")
    print(f"ğŸ“œ è…³æœ¬ç¸½é•·åº¦: {len(script)} å­—ç¬¦")
    print(f"ğŸ¤ èªªè©±è€…è²éŸ³: èªªè©±è€…1={speaker1_voice}, èªªè©±è€…2={speaker2_voice}")
    print(f"ğŸ”Š éŸ³é‡å¢å¼·: {volume_boost} dB")
    
    status_log = []
    
    # å„ªåŒ–è…³æœ¬è™•ç†
    print("ğŸ” å„ªåŒ–è…³æœ¬å…§å®¹...")
    optimized_script = optimize_script(script)
    print(f"âœ… è…³æœ¬å„ªåŒ–å®Œæˆï¼Œå…± {len(optimized_script)} å€‹ç‰‡æ®µ")
    
    # ä½¿ç”¨ pydub è™•ç†éŸ³é »åˆä¸¦
    combined_segment = None
    
    # è™•ç†æ¯ä¸€æ®µ
    total_segments = len(optimized_script)
    print(f"ğŸµ é–‹å§‹è™•ç† {total_segments} å€‹éŸ³é »ç‰‡æ®µ")
    
    for i, (speaker, text) in enumerate(optimized_script, 1):
        voice_to_use = speaker1_voice if speaker == "speaker-1" else speaker2_voice
        instructions_to_use = speaker1_instructions if speaker == "speaker-1" else speaker2_instructions
        
        print(f"ğŸ­ è™•ç†ç‰‡æ®µ {i}/{total_segments}: {speaker} ({len(text)} å­—ç¬¦)")
        status_log.append(f"[{speaker}] {text}")
        
        try:
            # ç”Ÿæˆé€™ä¸€æ®µçš„éŸ³é »
            print(f"ğŸ“¡ ç”Ÿæˆ {speaker} çš„éŸ³é »...")
            audio_chunk = get_mp3(
                text,
                voice_to_use,
                audio_model,
                audio_api_key,
                instructions_to_use
            )
            
            print(f"âœ… {speaker} éŸ³é »ç”Ÿæˆå®Œæˆ: {len(audio_chunk)} bytes")
            
            # å°‡äºŒé€²åˆ¶æ•¸æ“šè½‰æ›ç‚º AudioSegment
            with NamedTemporaryFile(suffix=".mp3", delete=False) as temp_file:
                temp_file.write(audio_chunk)
                temp_file_path = temp_file.name
            
            # è®€å–éŸ³é »
            chunk_segment = AudioSegment.from_mp3(temp_file_path)
            
            # åˆªé™¤è‡¨æ™‚æ–‡ä»¶
            os.unlink(temp_file_path)
            
            # åˆä¸¦éŸ³é »æ®µ
            if combined_segment is None:
                combined_segment = chunk_segment
                print("ğŸ”— å‰µå»ºç¬¬ä¸€å€‹éŸ³é »ç‰‡æ®µ")
            else:
                combined_segment += chunk_segment
                print(f"ğŸ”— å·²åˆä¸¦ç‰‡æ®µ {i}/{total_segments}")
                
        except Exception as e:
            error_msg = f"âŒ ç‰‡æ®µ {i} ({speaker}) ç”Ÿæˆå¤±æ•—: {str(e)}"
            print(error_msg)
            status_log.append(f"[éŒ¯èª¤] ç„¡æ³•ç”ŸæˆéŸ³é »: {str(e)}")
            raise
    
    # å¦‚æœæ²’æœ‰ç”Ÿæˆä»»ä½•éŸ³é »æ®µ
    if combined_segment is None:
        error_msg = "âŒ æ²’æœ‰ç”Ÿæˆä»»ä½•éŸ³é »"
        print(error_msg)
        status_log.append("[éŒ¯èª¤] æ²’æœ‰ç”Ÿæˆä»»ä½•éŸ³é »")
        return b"", "\n".join(status_log)
    
    # å¦‚æœéœ€è¦èª¿æ•´éŸ³é‡
    if volume_boost > 0:
        try:
            print(f"ğŸ”Š èª¿æ•´éŸ³é‡ +{volume_boost} dB...")
            # èª¿æ•´éŸ³é‡
            combined_segment = combined_segment + volume_boost  # å¢åŠ éŸ³é‡ (dB)
            status_log.append(f"[éŸ³é‡] å·²å¢åŠ  {volume_boost} dB")
            print("âœ… éŸ³é‡èª¿æ•´å®Œæˆ")
        except Exception as e:
            warning_msg = f"âš ï¸ éŸ³é‡èª¿æ•´å¤±æ•—: {str(e)}"
            print(warning_msg)
            status_log.append(f"[è­¦å‘Š] éŸ³é‡èª¿æ•´å¤±æ•—: {str(e)}")
    
    # å°‡ AudioSegment è½‰æ›ç‚ºäºŒé€²åˆ¶æ•¸æ“š
    print("ğŸ’¾ å°å‡ºæœ€çµ‚éŸ³é »æ–‡ä»¶...")
    output = io.BytesIO()
    combined_segment.export(output, format="mp3")
    combined_audio = output.getvalue()
    
    print(f"ğŸ‰ è…³æœ¬éŸ³é »ç”Ÿæˆå®Œæˆï¼æœ€çµ‚å¤§å°: {len(combined_audio)} bytes")
    return combined_audio, "\n".join(status_log)

def save_audio_file(audio_data: bytes) -> str:
    """å°‡éŸ³é »æ•¸æ“šä¿å­˜ç‚ºè‡¨æ™‚æ–‡ä»¶"""
    print("ğŸ’¾ é–‹å§‹ä¿å­˜éŸ³é »æ–‡ä»¶...")
    
    temp_dir = Path("./temp_audio")
    temp_dir.mkdir(exist_ok=True)
    
    # æ¸…ç†èˆŠæ–‡ä»¶
    old_files_count = 0
    for old_file in temp_dir.glob("*.mp3"):
        if old_file.stat().st_mtime < (time.time() - 24*60*60):  # 24å°æ™‚å‰çš„æ–‡ä»¶
            old_file.unlink()
            old_files_count += 1
    
    if old_files_count > 0:
        print(f"ğŸ§¹ æ¸…ç†äº† {old_files_count} å€‹èˆŠçš„è‡¨æ™‚æ–‡ä»¶")
    
    # å‰µå»ºæ–°çš„è‡¨æ™‚æ–‡ä»¶
    temp_file = NamedTemporaryFile(
        dir=temp_dir,
        delete=False,
        suffix=".mp3"
    )
    temp_file.write(audio_data)
    temp_file.close()
    
    print(f"âœ… éŸ³é »æ–‡ä»¶å·²ä¿å­˜: {temp_file.name} ({len(audio_data)} bytes)")
    return temp_file.name

def process_and_save_audio(script, api_key, model, voice1, voice2, volume_boost, instr1, instr2):
    """è™•ç†éŸ³é »ç”Ÿæˆä¸¦ä¿å­˜æ–‡ä»¶"""
    try:
        audio_data, status_log = generate_audio_from_script(
            script,
            api_key,
            model,
            voice1,
            voice2,
            volume_boost,
            instr1,
            instr2
        )
        audio_path = save_audio_file(audio_data)
        return audio_path, status_log
    except Exception as e:
        error_message = f"ç”ŸæˆéŸ³é »æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        print(error_message)
        return None, error_message

# Gradio ç•Œé¢
def create_gradio_interface():
    with gr.Blocks(title="TTS Generator", css="""
        #header { text-align: center; margin-bottom: 20px; }
    """) as demo:
        gr.Markdown("# èªéŸ³åˆæˆå™¨ | TTS Generator", elem_id="header")
        with gr.Row():
            with gr.Column(scale=1):
                # è¼¸å…¥å€
                script_input = gr.Textbox(
                    label="è¼¸å…¥è…³æœ¬ | Input Script",
                    placeholder="""è«‹ç²˜è²¼è…³æœ¬å…§å®¹ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
speaker-1: æ­¡è¿ä¾†åˆ° David888 Podcastï¼Œæˆ‘æ˜¯ David...
speaker-2: å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯ Cordelia...
æ²’æœ‰æ¨™è¨˜èªªè©±è€…çš„è¡Œæœƒé»˜èªä½¿ç”¨èªªè©±è€…1çš„è²éŸ³ã€‚

æç¤ºï¼šç‚ºæé«˜æ•ˆç‡ï¼Œç›¸åŒèªªè©±è€…çš„å¤šè¡Œæ–‡å­—å°‡è‡ªå‹•åˆä¸¦è™•ç†ã€‚""",
                    lines=20
                )
                api_key = gr.Textbox(
                    label="OpenAI API Key",
                    type="password"
                )
                with gr.Row():
                    audio_model = gr.Dropdown(
                        label="éŸ³é »æ¨¡å‹ | Audio Model",
                        choices=STANDARD_AUDIO_MODELS,
                        value="gpt-4o-mini-tts"
                    )
                    speaker1_voice = gr.Dropdown(
                        label="èªªè©±è€…1è²éŸ³ (ç”·è§’) | Speaker 1 Voice (Male)",
                        choices=STANDARD_VOICES,
                        value="onyx"
                    )
                    speaker2_voice = gr.Dropdown(
                        label="èªªè©±è€…2è²éŸ³ (å¥³è§’) | Speaker 2 Voice (Female)",
                        choices=STANDARD_VOICES,
                        value="nova"
                    )
                
                with gr.Row():
                    speaker1_instructions = gr.Textbox(
                        label="èªªè©±è€…1èªæ°£ | Speaker 1 Instructions",
                        value="ä¿æŒæ´»æ½‘æ„‰å¿«çš„èªæ°£",
                        placeholder="ä¾‹å¦‚:ä¿æŒæ´»æ½‘æ„‰å¿«çš„èªæ°£ã€ç”¨å°ˆæ¥­åš´è‚…çš„å£å»èªªè©±ç­‰"
                    )
                    speaker2_instructions = gr.Textbox(
                        label="èªªè©±è€…2èªæ°£ | Speaker 2 Instructions",
                        value="ä¿æŒæ´»æ½‘æ„‰å¿«çš„èªæ°£",
                        placeholder="ä¾‹å¦‚:ä¿æŒæ´»æ½‘æ„‰å¿«çš„èªæ°£ã€ç”¨å°ˆæ¥­åš´è‚…çš„å£å»èªªè©±ç­‰"
                    )
                
                volume_boost = gr.Slider(
                    label="éŸ³é‡å¢ç›Š (dB) | Volume Boost (dB)",
                    minimum=0,
                    maximum=20,
                    value=6,
                    step=1,
                    info="å¢åŠ éŸ³é »éŸ³é‡ï¼Œå–®ä½ç‚ºåˆ†è²(dB)ã€‚å»ºè­°å€¼ï¼š6-10 dB"
                )
                generate_button = gr.Button("ç”ŸæˆéŸ³é » | Generate Audio")
            with gr.Column(scale=1):
                # è¼¸å‡ºå€
                audio_output = gr.Audio(
                    label="ç”Ÿæˆçš„éŸ³é » | Generated Audio",
                    type="filepath"
                )
                status_output = gr.Textbox(
                    label="ç”Ÿæˆç‹€æ…‹ | Generation Status",
                    lines=20,
                    show_copy_button=True
                )
        
        # äº‹ä»¶è™•ç†
        generate_button.click(
            fn=process_and_save_audio,
            inputs=[
                script_input,
                api_key,
                audio_model,
                speaker1_voice,
                speaker2_voice,
                volume_boost,
                speaker1_instructions,
                speaker2_instructions
            ],
            outputs=[audio_output, status_output]
        )
    return demo


demo = create_gradio_interface()
# Hugging Face Spaces expects a global `app` Gradio interface
app = demo.queue()

if __name__ == "__main__":
    app.launch(server_name="0.0.0.0", server_port=7860)